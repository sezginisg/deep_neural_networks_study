{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Regression MLP Using the Sequintial API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will tackle with California housing problem using a regression neural network. We will use Scikit-Learn's fetch_california_housing() function to load the data. This data contains only numerical features (there is no ocean_proximity feature), and there is no missing value. After loading the data;\n",
    "\n",
    "-we split it into training set, a validation set, and a test set \n",
    "\n",
    "-and we scale all the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the data\n",
    "housing = fetch_california_housing()\n",
    "#splitting into train, validation and test sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\n",
    "#scale all the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Sequential API to build, train, evaluate, and use a regression MLP to make prediction is quite similar to what we did for classification. The main differences are the fact the output layer has a single neuron (since we only want to predict a single value) and uses no activation function, and the loss function is the mean squared error. Since the dataset is quite noisy, we just use a single hidden layer with fewer neurons than before, to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu', input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 1s 79us/sample - loss: 0.9126 - val_loss: 0.5993\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 1s 59us/sample - loss: 0.5570 - val_loss: 0.5174\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 1s 57us/sample - loss: 0.4967 - val_loss: 0.4826\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 1s 57us/sample - loss: 0.4647 - val_loss: 0.4677\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 1s 58us/sample - loss: 0.4539 - val_loss: 0.4543\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 1s 69us/sample - loss: 0.4411 - val_loss: 0.4469\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 1s 68us/sample - loss: 0.4312 - val_loss: 0.4376\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 1s 57us/sample - loss: 0.4292 - val_loss: 0.4348\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 1s 57us/sample - loss: 0.4217 - val_loss: 0.4539\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 1s 58us/sample - loss: 0.4329 - val_loss: 0.4260\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 1s 57us/sample - loss: 0.4112 - val_loss: 0.4210\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 1s 57us/sample - loss: 0.4103 - val_loss: 0.4110\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 1s 58us/sample - loss: 0.4095 - val_loss: 0.4079\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 1s 69us/sample - loss: 0.4175 - val_loss: 0.4156\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 1s 69us/sample - loss: 0.4038 - val_loss: 0.4189\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 1s 63us/sample - loss: 0.4001 - val_loss: 0.4114\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 1s 58us/sample - loss: 0.3950 - val_loss: 0.4018\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 1s 58us/sample - loss: 0.3920 - val_loss: 0.3940\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 1s 59us/sample - loss: 0.3888 - val_loss: 0.4007\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 1s 58us/sample - loss: 0.3894 - val_loss: 0.3930\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5160/5160 [==============================] - 0s 31us/sample - loss: 0.3937\n"
     ]
    }
   ],
   "source": [
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_test[:3]\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.217 2.38  2.667] \n",
      "\n",
      "\n",
      "[[1.2867423]\n",
      " [3.127782 ]\n",
      " [3.0932076]]\n"
     ]
    }
   ],
   "source": [
    "print(y_test[:3], '\\n\\n\\n' + str(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
