{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Restoring a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the Sequential API or the Functional API, saving a trained Keras model is as simple as it gets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000]/255.0, X_train_full[25000:]/255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[25000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape = X_train.shape[1:]),\n",
    "    keras.layers.Dense(300, activation='relu'),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35000 samples, validate on 5000 samples\n",
      "Epoch 1/30\n",
      "35000/35000 [==============================] - 4s 104us/sample - loss: 0.8233 - accuracy: 0.7355 - val_loss: 0.5602 - val_accuracy: 0.8136\n",
      "Epoch 2/30\n",
      "35000/35000 [==============================] - 3s 94us/sample - loss: 0.5310 - accuracy: 0.8194 - val_loss: 0.4994 - val_accuracy: 0.8210\n",
      "Epoch 3/30\n",
      "35000/35000 [==============================] - 3s 92us/sample - loss: 0.4751 - accuracy: 0.8354 - val_loss: 0.4615 - val_accuracy: 0.8400\n",
      "Epoch 4/30\n",
      "35000/35000 [==============================] - 4s 111us/sample - loss: 0.4479 - accuracy: 0.8453 - val_loss: 0.4325 - val_accuracy: 0.8556\n",
      "Epoch 5/30\n",
      "35000/35000 [==============================] - 4s 104us/sample - loss: 0.4261 - accuracy: 0.8518 - val_loss: 0.4170 - val_accuracy: 0.8526\n",
      "Epoch 6/30\n",
      "35000/35000 [==============================] - 3s 96us/sample - loss: 0.4076 - accuracy: 0.8572 - val_loss: 0.4226 - val_accuracy: 0.8572\n",
      "Epoch 7/30\n",
      "35000/35000 [==============================] - 3s 94us/sample - loss: 0.3941 - accuracy: 0.8623 - val_loss: 0.4226 - val_accuracy: 0.8548\n",
      "Epoch 8/30\n",
      "35000/35000 [==============================] - 3s 95us/sample - loss: 0.3801 - accuracy: 0.8685 - val_loss: 0.4155 - val_accuracy: 0.8502\n",
      "Epoch 9/30\n",
      "35000/35000 [==============================] - 3s 93us/sample - loss: 0.3709 - accuracy: 0.8720 - val_loss: 0.3807 - val_accuracy: 0.8642\n",
      "Epoch 10/30\n",
      "35000/35000 [==============================] - 4s 103us/sample - loss: 0.3608 - accuracy: 0.8727 - val_loss: 0.3683 - val_accuracy: 0.8650\n",
      "Epoch 11/30\n",
      "35000/35000 [==============================] - 3s 96us/sample - loss: 0.3519 - accuracy: 0.8767 - val_loss: 0.3746 - val_accuracy: 0.8692\n",
      "Epoch 12/30\n",
      "35000/35000 [==============================] - 3s 98us/sample - loss: 0.3432 - accuracy: 0.8809 - val_loss: 0.3625 - val_accuracy: 0.8714\n",
      "Epoch 13/30\n",
      "35000/35000 [==============================] - 4s 100us/sample - loss: 0.3346 - accuracy: 0.8827 - val_loss: 0.3603 - val_accuracy: 0.8730\n",
      "Epoch 14/30\n",
      "35000/35000 [==============================] - 4s 106us/sample - loss: 0.3286 - accuracy: 0.8851 - val_loss: 0.3553 - val_accuracy: 0.8736\n",
      "Epoch 15/30\n",
      "35000/35000 [==============================] - 4s 115us/sample - loss: 0.3215 - accuracy: 0.8864 - val_loss: 0.3563 - val_accuracy: 0.8704\n",
      "Epoch 16/30\n",
      "35000/35000 [==============================] - 4s 100us/sample - loss: 0.3141 - accuracy: 0.8888 - val_loss: 0.3530 - val_accuracy: 0.8764\n",
      "Epoch 17/30\n",
      "35000/35000 [==============================] - 3s 96us/sample - loss: 0.3095 - accuracy: 0.8915 - val_loss: 0.3388 - val_accuracy: 0.8798\n",
      "Epoch 18/30\n",
      "35000/35000 [==============================] - 4s 117us/sample - loss: 0.3040 - accuracy: 0.8929 - val_loss: 0.3368 - val_accuracy: 0.8794\n",
      "Epoch 19/30\n",
      "35000/35000 [==============================] - 4s 102us/sample - loss: 0.2973 - accuracy: 0.8936 - val_loss: 0.3502 - val_accuracy: 0.8768\n",
      "Epoch 20/30\n",
      "35000/35000 [==============================] - 3s 97us/sample - loss: 0.2934 - accuracy: 0.8967 - val_loss: 0.3399 - val_accuracy: 0.8776\n",
      "Epoch 21/30\n",
      "35000/35000 [==============================] - 4s 100us/sample - loss: 0.2869 - accuracy: 0.8978 - val_loss: 0.3387 - val_accuracy: 0.8802\n",
      "Epoch 22/30\n",
      "35000/35000 [==============================] - 3s 98us/sample - loss: 0.2828 - accuracy: 0.9003 - val_loss: 0.3286 - val_accuracy: 0.8800\n",
      "Epoch 23/30\n",
      "35000/35000 [==============================] - 3s 100us/sample - loss: 0.2763 - accuracy: 0.9023 - val_loss: 0.3482 - val_accuracy: 0.8752\n",
      "Epoch 24/30\n",
      "35000/35000 [==============================] - 3s 98us/sample - loss: 0.2722 - accuracy: 0.9029 - val_loss: 0.3558 - val_accuracy: 0.8724\n",
      "Epoch 25/30\n",
      "35000/35000 [==============================] - 3s 98us/sample - loss: 0.2667 - accuracy: 0.9049 - val_loss: 0.3330 - val_accuracy: 0.8792\n",
      "Epoch 26/30\n",
      "35000/35000 [==============================] - 4s 100us/sample - loss: 0.2628 - accuracy: 0.9066 - val_loss: 0.3310 - val_accuracy: 0.8812\n",
      "Epoch 27/30\n",
      "35000/35000 [==============================] - 3s 99us/sample - loss: 0.2586 - accuracy: 0.9072 - val_loss: 0.3273 - val_accuracy: 0.8854\n",
      "Epoch 28/30\n",
      "35000/35000 [==============================] - 4s 101us/sample - loss: 0.2543 - accuracy: 0.9097 - val_loss: 0.3259 - val_accuracy: 0.8856\n",
      "Epoch 29/30\n",
      "35000/35000 [==============================] - 4s 119us/sample - loss: 0.2505 - accuracy: 0.9105 - val_loss: 0.3317 - val_accuracy: 0.8790\n",
      "Epoch 30/30\n",
      "35000/35000 [==============================] - 5s 129us/sample - loss: 0.2463 - accuracy: 0.9121 - val_loss: 0.3246 - val_accuracy: 0.8838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14cc92160>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_keras_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras will use the HDF5 format to save both the model's arhitecture (including every layer's hyperparameters) and the values of all the model parameters for every layer (e.g., connection weights and biases). It also saves the optimizer (including its hyperparameters and any state it may have). You will typically have a script that trains a model and saves it, and one or more scripts (or web services) that load the model and use it to make predictions. Loading the model is just as easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('my_keras_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will work when using the Sequential Api or the Fucntional API, but unfortunately not when using model subclassing. You can use save_weights() and load_weights() to at least save and restore the model parameters, but you will need to save and restore everything yourself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if training lasts several hours? This is quite common, especially when training on large datasets. In this case, you should not only save your model at the end of training , but also save checkpoints at regular intervals during training, to avoid losing everything if your computer crashes. But how can you tell the fit() method to save checkpoints? Use callbacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit() method accepts a callbacks argument that lets you specify a list of objects that Keras will call at the start and at the end of training , at the start and end of each epoch, and even before and after processing each batch. For example, the ModelCheckpoint callback saves checkpoints of your model at regular intervals during training, by default at the end of each epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35000 samples\n",
      "Epoch 1/10\n",
      "35000/35000 [==============================] - 4s 122us/sample - loss: 0.8165 - accuracy: 0.7365\n",
      "Epoch 2/10\n",
      "35000/35000 [==============================] - 3s 97us/sample - loss: 0.5304 - accuracy: 0.8193\n",
      "Epoch 3/10\n",
      "35000/35000 [==============================] - 4s 104us/sample - loss: 0.4813 - accuracy: 0.8333\n",
      "Epoch 4/10\n",
      "35000/35000 [==============================] - 4s 119us/sample - loss: 0.4521 - accuracy: 0.8427\n",
      "Epoch 5/10\n",
      "35000/35000 [==============================] - 3s 95us/sample - loss: 0.4300 - accuracy: 0.8522\n",
      "Epoch 6/10\n",
      "35000/35000 [==============================] - 3s 81us/sample - loss: 0.4119 - accuracy: 0.8570s - loss: 0.4109 - accu\n",
      "Epoch 7/10\n",
      "35000/35000 [==============================] - 3s 85us/sample - loss: 0.3967 - accuracy: 0.8618\n",
      "Epoch 8/10\n",
      "35000/35000 [==============================] - 3s 83us/sample - loss: 0.3854 - accuracy: 0.8647\n",
      "Epoch 9/10\n",
      "35000/35000 [==============================] - 3s 88us/sample - loss: 0.3727 - accuracy: 0.8711\n",
      "Epoch 10/10\n",
      "35000/35000 [==============================] - 3s 85us/sample - loss: 0.3632 - accuracy: 0.8732\n"
     ]
    }
   ],
   "source": [
    "#build and compile the model\n",
    "model_0 = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape = X_train.shape[1:]),\n",
    "    keras.layers.Dense(300, activation='relu'),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model_0.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model_0.h5\")\n",
    "history_0 = model_0.fit(X_train, y_train, epochs=10, callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Morove if you use a validation set during training you can use save_best_only = True when creating the ModelCheckpoint. In this case, it will only save your model when it's performance on the validation set is the best so far. This way, you do not need to worry about training for too long and overfitting the training set: simply restore the last model saved after training, and this will be the best model on the validation set. The following code is simple way to implement early stopping:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "35000/35000 [==============================] - 3s 100us/sample - loss: 0.8160 - accuracy: 0.7424 - val_loss: 0.6027 - val_accuracy: 0.7934\n",
      "Epoch 2/10\n",
      "35000/35000 [==============================] - 3s 89us/sample - loss: 0.5281 - accuracy: 0.8212 - val_loss: 0.4923 - val_accuracy: 0.8338\n",
      "Epoch 3/10\n",
      "35000/35000 [==============================] - 3s 89us/sample - loss: 0.4773 - accuracy: 0.8339 - val_loss: 0.4476 - val_accuracy: 0.8466\n",
      "Epoch 4/10\n",
      "35000/35000 [==============================] - 3s 91us/sample - loss: 0.4461 - accuracy: 0.8454 - val_loss: 0.4547 - val_accuracy: 0.8416\n",
      "Epoch 5/10\n",
      "35000/35000 [==============================] - 3s 90us/sample - loss: 0.4271 - accuracy: 0.8514 - val_loss: 0.4138 - val_accuracy: 0.8564\n",
      "Epoch 6/10\n",
      "35000/35000 [==============================] - 3s 88us/sample - loss: 0.4085 - accuracy: 0.8576 - val_loss: 0.4165 - val_accuracy: 0.8592\n",
      "Epoch 7/10\n",
      "35000/35000 [==============================] - 3s 96us/sample - loss: 0.3950 - accuracy: 0.8633 - val_loss: 0.4025 - val_accuracy: 0.8610\n",
      "Epoch 8/10\n",
      "35000/35000 [==============================] - 3s 94us/sample - loss: 0.3834 - accuracy: 0.8667 - val_loss: 0.3778 - val_accuracy: 0.8684\n",
      "Epoch 9/10\n",
      "35000/35000 [==============================] - 3s 88us/sample - loss: 0.3722 - accuracy: 0.8697 - val_loss: 0.3919 - val_accuracy: 0.8646\n",
      "Epoch 10/10\n",
      "35000/35000 [==============================] - 3s 90us/sample - loss: 0.3637 - accuracy: 0.8700 - val_loss: 0.3641 - val_accuracy: 0.8760\n"
     ]
    }
   ],
   "source": [
    "model_1 = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape = X_train.shape[1:]),\n",
    "    keras.layers.Dense(300, activation='relu'),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model_1.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('my_keras_model_1.h5', save_best_only=True)\n",
    "history_1 = model_1.fit(X_train, y_train,epochs=10,validation_data=(X_valid, y_valid),callbacks=[checkpoint_cb])\n",
    "model = keras.models.load_model('my_keras_model_1.h5') #roll back to best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to implement early stopping is to simply use the EarlyStopping callback. It will interrupt training when it measures no progress on the validation set for a number of epoch (defined by the patience argument), and it will optionally roll back to the best model. You can combine both callbacks to checkpoints of your model (in case your computer crashes) and interrupt training early when there is no more progress (to avoid wasting time and resources):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "35000/35000 [==============================] - 4s 106us/sample - loss: 0.7840 - accuracy: 0.7497 - val_loss: 0.5675 - val_accuracy: 0.8082\n",
      "Epoch 2/100\n",
      "35000/35000 [==============================] - 4s 108us/sample - loss: 0.5192 - accuracy: 0.8233 - val_loss: 0.5012 - val_accuracy: 0.8292\n",
      "Epoch 3/100\n",
      "35000/35000 [==============================] - 4s 104us/sample - loss: 0.4695 - accuracy: 0.8362 - val_loss: 0.4539 - val_accuracy: 0.8434\n",
      "Epoch 4/100\n",
      "35000/35000 [==============================] - 4s 101us/sample - loss: 0.4400 - accuracy: 0.8459 - val_loss: 0.4248 - val_accuracy: 0.8580\n",
      "Epoch 5/100\n",
      "35000/35000 [==============================] - 3s 99us/sample - loss: 0.4185 - accuracy: 0.8551 - val_loss: 0.4213 - val_accuracy: 0.8570\n",
      "Epoch 6/100\n",
      "35000/35000 [==============================] - 4s 101us/sample - loss: 0.4025 - accuracy: 0.8597 - val_loss: 0.3914 - val_accuracy: 0.8694\n",
      "Epoch 7/100\n",
      "35000/35000 [==============================] - 4s 106us/sample - loss: 0.3874 - accuracy: 0.8639 - val_loss: 0.4020 - val_accuracy: 0.8664\n",
      "Epoch 8/100\n",
      "35000/35000 [==============================] - 4s 101us/sample - loss: 0.3784 - accuracy: 0.8663 - val_loss: 0.3846 - val_accuracy: 0.8654\n",
      "Epoch 9/100\n",
      "35000/35000 [==============================] - 4s 102us/sample - loss: 0.3658 - accuracy: 0.8715 - val_loss: 0.3745 - val_accuracy: 0.8722\n",
      "Epoch 10/100\n",
      "35000/35000 [==============================] - 4s 103us/sample - loss: 0.3582 - accuracy: 0.8744 - val_loss: 0.3738 - val_accuracy: 0.8698\n",
      "Epoch 11/100\n",
      "35000/35000 [==============================] - 4s 103us/sample - loss: 0.3479 - accuracy: 0.8771 - val_loss: 0.3783 - val_accuracy: 0.8718\n",
      "Epoch 12/100\n",
      "35000/35000 [==============================] - 4s 110us/sample - loss: 0.3407 - accuracy: 0.8797 - val_loss: 0.3826 - val_accuracy: 0.8650\n",
      "Epoch 13/100\n",
      "35000/35000 [==============================] - 4s 106us/sample - loss: 0.3338 - accuracy: 0.8829 - val_loss: 0.3878 - val_accuracy: 0.8678\n",
      "Epoch 14/100\n",
      "35000/35000 [==============================] - 4s 106us/sample - loss: 0.3250 - accuracy: 0.8869 - val_loss: 0.3641 - val_accuracy: 0.8740\n",
      "Epoch 15/100\n",
      "35000/35000 [==============================] - 4s 110us/sample - loss: 0.3184 - accuracy: 0.8870 - val_loss: 0.3458 - val_accuracy: 0.8800\n",
      "Epoch 16/100\n",
      "35000/35000 [==============================] - 4s 112us/sample - loss: 0.3126 - accuracy: 0.8910 - val_loss: 0.3564 - val_accuracy: 0.8774\n",
      "Epoch 17/100\n",
      "35000/35000 [==============================] - 4s 112us/sample - loss: 0.3070 - accuracy: 0.8908 - val_loss: 0.3454 - val_accuracy: 0.8766\n",
      "Epoch 18/100\n",
      "35000/35000 [==============================] - 4s 106us/sample - loss: 0.3012 - accuracy: 0.8931 - val_loss: 0.3496 - val_accuracy: 0.8746\n",
      "Epoch 19/100\n",
      "35000/35000 [==============================] - 4s 106us/sample - loss: 0.2961 - accuracy: 0.8940 - val_loss: 0.3505 - val_accuracy: 0.8784\n",
      "Epoch 20/100\n",
      "35000/35000 [==============================] - 4s 107us/sample - loss: 0.2910 - accuracy: 0.8969 - val_loss: 0.3324 - val_accuracy: 0.8808\n",
      "Epoch 21/100\n",
      "35000/35000 [==============================] - 4s 108us/sample - loss: 0.2857 - accuracy: 0.8990 - val_loss: 0.3434 - val_accuracy: 0.8756\n",
      "Epoch 22/100\n",
      "35000/35000 [==============================] - 4s 113us/sample - loss: 0.2796 - accuracy: 0.9016 - val_loss: 0.3274 - val_accuracy: 0.8836\n",
      "Epoch 23/100\n",
      "35000/35000 [==============================] - 4s 102us/sample - loss: 0.2756 - accuracy: 0.9010 - val_loss: 0.3318 - val_accuracy: 0.8832\n",
      "Epoch 24/100\n",
      "35000/35000 [==============================] - 4s 106us/sample - loss: 0.2707 - accuracy: 0.9053 - val_loss: 0.3298 - val_accuracy: 0.8840\n",
      "Epoch 25/100\n",
      "35000/35000 [==============================] - 4s 102us/sample - loss: 0.2656 - accuracy: 0.9051 - val_loss: 0.3309 - val_accuracy: 0.8854\n",
      "Epoch 26/100\n",
      "35000/35000 [==============================] - 4s 104us/sample - loss: 0.2620 - accuracy: 0.9075 - val_loss: 0.3348 - val_accuracy: 0.8788\n",
      "Epoch 27/100\n",
      "35000/35000 [==============================] - 4s 105us/sample - loss: 0.2587 - accuracy: 0.9068 - val_loss: 0.3226 - val_accuracy: 0.8878\n",
      "Epoch 28/100\n",
      "35000/35000 [==============================] - 4s 103us/sample - loss: 0.2547 - accuracy: 0.9100 - val_loss: 0.3333 - val_accuracy: 0.8822\n",
      "Epoch 29/100\n",
      "35000/35000 [==============================] - 4s 107us/sample - loss: 0.2509 - accuracy: 0.9113 - val_loss: 0.3308 - val_accuracy: 0.8844\n",
      "Epoch 30/100\n",
      "35000/35000 [==============================] - 4s 127us/sample - loss: 0.2461 - accuracy: 0.9135 - val_loss: 0.3187 - val_accuracy: 0.8900\n",
      "Epoch 31/100\n",
      "35000/35000 [==============================] - 4s 118us/sample - loss: 0.2427 - accuracy: 0.9148 - val_loss: 0.3346 - val_accuracy: 0.8828\n",
      "Epoch 32/100\n",
      "35000/35000 [==============================] - 4s 117us/sample - loss: 0.2388 - accuracy: 0.9163 - val_loss: 0.3213 - val_accuracy: 0.8854\n",
      "Epoch 33/100\n",
      "35000/35000 [==============================] - 4s 108us/sample - loss: 0.2352 - accuracy: 0.9180 - val_loss: 0.3168 - val_accuracy: 0.8890\n",
      "Epoch 34/100\n",
      "35000/35000 [==============================] - 4s 108us/sample - loss: 0.2318 - accuracy: 0.9173 - val_loss: 0.3381 - val_accuracy: 0.8834\n",
      "Epoch 35/100\n",
      "35000/35000 [==============================] - 4s 110us/sample - loss: 0.2257 - accuracy: 0.9206 - val_loss: 0.3726 - val_accuracy: 0.8618\n",
      "Epoch 36/100\n",
      "35000/35000 [==============================] - 4s 122us/sample - loss: 0.2249 - accuracy: 0.9213 - val_loss: 0.3156 - val_accuracy: 0.8898\n",
      "Epoch 37/100\n",
      "35000/35000 [==============================] - 4s 108us/sample - loss: 0.2206 - accuracy: 0.9218 - val_loss: 0.3336 - val_accuracy: 0.8756\n",
      "Epoch 38/100\n",
      "35000/35000 [==============================] - 4s 115us/sample - loss: 0.2168 - accuracy: 0.9235 - val_loss: 0.3175 - val_accuracy: 0.8910\n",
      "Epoch 39/100\n",
      "35000/35000 [==============================] - 4s 112us/sample - loss: 0.2141 - accuracy: 0.9252 - val_loss: 0.3137 - val_accuracy: 0.8888\n",
      "Epoch 40/100\n",
      "35000/35000 [==============================] - 4s 110us/sample - loss: 0.2105 - accuracy: 0.9251 - val_loss: 0.3374 - val_accuracy: 0.8806\n",
      "Epoch 41/100\n",
      "35000/35000 [==============================] - 4s 114us/sample - loss: 0.2084 - accuracy: 0.9269 - val_loss: 0.3210 - val_accuracy: 0.8874\n",
      "Epoch 42/100\n",
      "35000/35000 [==============================] - 4s 119us/sample - loss: 0.2047 - accuracy: 0.9271 - val_loss: 0.3117 - val_accuracy: 0.8908\n",
      "Epoch 43/100\n",
      "35000/35000 [==============================] - 4s 119us/sample - loss: 0.2017 - accuracy: 0.9285 - val_loss: 0.3127 - val_accuracy: 0.8888\n",
      "Epoch 44/100\n",
      "35000/35000 [==============================] - 5s 131us/sample - loss: 0.1983 - accuracy: 0.9302 - val_loss: 0.3181 - val_accuracy: 0.8882\n",
      "Epoch 45/100\n",
      "35000/35000 [==============================] - 4s 127us/sample - loss: 0.1938 - accuracy: 0.9312 - val_loss: 0.3597 - val_accuracy: 0.8838\n",
      "Epoch 46/100\n",
      "35000/35000 [==============================] - 4s 120us/sample - loss: 0.1921 - accuracy: 0.9322 - val_loss: 0.3317 - val_accuracy: 0.8846\n",
      "Epoch 47/100\n",
      "35000/35000 [==============================] - 4s 106us/sample - loss: 0.1881 - accuracy: 0.9343 - val_loss: 0.3160 - val_accuracy: 0.8866\n",
      "Epoch 48/100\n",
      "35000/35000 [==============================] - 4s 112us/sample - loss: 0.1847 - accuracy: 0.9363 - val_loss: 0.3202 - val_accuracy: 0.8904\n",
      "Epoch 49/100\n",
      "35000/35000 [==============================] - 4s 111us/sample - loss: 0.1838 - accuracy: 0.9347 - val_loss: 0.3137 - val_accuracy: 0.8906\n",
      "Epoch 50/100\n",
      "35000/35000 [==============================] - 4s 104us/sample - loss: 0.1810 - accuracy: 0.9364 - val_loss: 0.3267 - val_accuracy: 0.8926\n",
      "Epoch 51/100\n",
      "35000/35000 [==============================] - 4s 109us/sample - loss: 0.1766 - accuracy: 0.9396 - val_loss: 0.3122 - val_accuracy: 0.8874\n",
      "Epoch 52/100\n",
      "35000/35000 [==============================] - 4s 108us/sample - loss: 0.1736 - accuracy: 0.9403 - val_loss: 0.3174 - val_accuracy: 0.8874\n"
     ]
    }
   ],
   "source": [
    "model_2 = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape = X_train.shape[1:]),\n",
    "    keras.layers.Dense(300, activation='relu'),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model_2.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "checkpoint_cb_2 = keras.callbacks.ModelCheckpoint('my_keras_model_2.h5', save_best_only=True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "history_2 = model_2.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid),\n",
    "                        callbacks=[checkpoint_cb_2, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of epochs can be set to a large value since training will stop automatically when there is no more progress. In this case, there is no need to restore the best model saved because the EarlyStopping callback will keep track of the best weights and restore them for you at the end of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is many other callbacks available in the keras.callbacks package (https://keras.io/callbacks/.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need extra control, you can easily write your own custom callbacks. As an example of how to do that, \n",
    "the following custom callback will display the ratio between the validation loss and training loss during training(e.g. to detect overfitting):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print('\\nval/train: {:2f}'.format(logs['val_loss']/logs['loss']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might expect you can implement on_train_begin(), on_train_end(), on_epoch_begin(), on_epoch_end(),on_batc_begin(),on_natch_end(). Callbacks can also be used during evaluation and predictions, should you ever need them on_test_end(), on_test_batch_begin(), or on_test_batch_end() (called by evaluate()), and for prediction you should implement on_predict_begin(), on_predict_end(), on_predict_batch_begin(), or on_predict_batch_end() (called by predict())."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at one more tool from tf.keras: TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TensorBoard for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
