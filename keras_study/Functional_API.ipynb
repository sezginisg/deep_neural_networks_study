{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Complex Models Using the Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although, Sequential models are extremely common, it is sometimes useful to build neural networks with more complex topologies, or with multiple inputs or outputs. For this purpose, Keras offers the Functional API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide & Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One example of nonsequential neural networks is a Wide and Deep neural network.It connects all or part of the inputs directly to the output layer. This architecture makes it possible for the neural network to learn  both deep patterns (using the deep path) and simple rules (through the short path). In contrast, a regular MLP forces all the data to flow through the full stack of layers, thus, simple patterns in the data may end up being distorted by this sequence of transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build such a neural network to tackle the California Housing problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the data\n",
    "housing = fetch_california_housing()\n",
    "#splitting into train, validation and test sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\n",
    "#scale all the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_)\n",
    "hidden2 = keras.layers.Dense(10, activation='relu')(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.Model(inputs = [input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to create an Input object. This is a specfication of the kind of input the model will get, including its shape and dtype. A model may actually have multiple inputs, as we will see shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a Dense layer with 30 neurons using the ReLu activation function. As soon as it is created, notice that we call it like a function, passing it the input. This is why it's called functional API. Note that we are just telling Keras how it should connect the layers together; no actual data is being processed yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a second hidden layer, and again we use it as a funtion. Note that we pass it the output of the first hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a Concatenate layer, and once again we immediately use it like a function, to concatenate the input and the output of the second hidden layer. You may prefer the keras.layers.concatenate() function which creates a Concatenate layer and immediately calls it with the given inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create the output layer, with a single neuron and no activation function and we call it like a function, passing it the result of the concatenation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we create a Keras Model, specifying which inputs and outputs to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have built the Keras model, you must compile the model, train it, evaluate it, and use it to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if you want to send a subset of the features through the wide path and a different subset (possibly overlapping) throuht the deep path. In this case, one solution is to use multiple inputs. For example, supoose we want to send five features through the wide path (features 0-4), and six features through the deep path(features2-7):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name='wide-input')\n",
    "input_B = keras.layers.Input(shape=[6],name='deep_input')\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name='output')(concat)\n",
    "model = keras.Model(inputs=[input_A, input_B],outputs = [output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is self explanatory. You should name at least the most important layers, especially when the model get a bit complex like this. Note that we specified inputs=[input_A, inpu_B] when creating the model. Now we can compile the model as usual, but when we call the fit() method, instead of passing a single input matrix X_train, we pass a pair of metrices (X_train_A, X_train_B): one per input. The same is true for X_valid, and also for X_test and X_new when you call evaluate() or predict():\n",
    "\n",
    "(Alternatively you can pass a dictionary mapping the input names to the input values, like ('wide_input':X_train_A, 'deep_input': X_train_B) This is especially useful when there are many inputs, to avoid getting the order wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 2s 130us/sample - loss: 1.8941 - val_loss: 0.9002\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 1s 78us/sample - loss: 0.7724 - val_loss: 0.6986\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.6775 - val_loss: 0.6357\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 1s 64us/sample - loss: 0.6299 - val_loss: 0.5963\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 1s 64us/sample - loss: 0.5958 - val_loss: 0.5674\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 1s 64us/sample - loss: 0.5693 - val_loss: 0.5436\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 1s 63us/sample - loss: 0.5483 - val_loss: 0.5254\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 1s 64us/sample - loss: 0.5311 - val_loss: 0.5105\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 1s 79us/sample - loss: 0.5173 - val_loss: 0.4986\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 1s 75us/sample - loss: 0.5062 - val_loss: 0.4892\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 1s 79us/sample - loss: 0.4974 - val_loss: 0.4795\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 1s 72us/sample - loss: 0.4906 - val_loss: 0.4723\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 1s 64us/sample - loss: 0.4847 - val_loss: 0.4663\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.4794 - val_loss: 0.4617\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 1s 65us/sample - loss: 0.4750 - val_loss: 0.4570\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 1s 78us/sample - loss: 0.4711 - val_loss: 0.4538\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 1s 74us/sample - loss: 0.4675 - val_loss: 0.4500\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 1s 67us/sample - loss: 0.4644 - val_loss: 0.4470\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 1s 65us/sample - loss: 0.4615 - val_loss: 0.4439\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 1s 73us/sample - loss: 0.4591 - val_loss: 0.4410\n"
     ]
    }
   ],
   "source": [
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=20, validation_data=((X_valid_A, X_valid_B), y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5160/5160 [==============================] - 0s 38us/sample - loss: 0.4474\n"
     ]
    }
   ],
   "source": [
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "y_pred = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.375 3.742 2.017] \n",
      "\n",
      "\n",
      "[[2.2951705]\n",
      " [1.8837408]\n",
      " [2.9152997]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:3], \"\\n\\n\")\n",
    "print((y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many cases which you may want to have multiple outputs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--The task may demand it. For instance, you may want to locate and classify the main object in a picture. This is both a regression task(finding the coordinates of the object's center, as well as its width and height) and a classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--Similarly, you may have multiple independent tasks based on the same data. Sure, you could train one neural network per task, but in many cases you will get better results on all tasks by training a single neural network with one output per task. This is because the neural network can learn features in the data that are useful across tasks. For example, you could perform multitask classification on pictures of faces, using one output to calssify person's facial expression (smiling, surprised etc.), and another output to identify whether they are wearing glasses or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--Another use case is as a regularization technique(i.e. a training constraints whose objective is to reduce overfitting and thus improve the model's ability to generalize). For example, you may want to add some auxiliary outputs in a neural network acrhitecture(see Figure 10-16)to ensure that the underlying part of the network learns something useful on its own, without relying on the rest of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding extra output is quite easy: just connect them to appropriate layers and add them to your model's list of outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name='wide-input')\n",
    "input_B = keras.layers.Input(shape=[6],name='deep_input')\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name='main_output')(concat)\n",
    "aux_output = keras.layers.Dense(1, name='aux_output')(hidden2)\n",
    "model = keras.Model(inputs = [input_A, input_B], outputs = [output, aux_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each output will need it's own loss function. Therefore, when we compile the model, we should pass a list of losses.(if we pass a single loss Keras will assume that the same loss should be used for all outputs). By default, Keras will compute all these losses and simply add them up to get the final loss used for training. We care much more about the main output than about the auxiliary output(as it is just used for regularization), so we want to give the main output's loss a much greater weight. Fortunately, it is possible to set all the loss weights when compiling the model.\n",
    "\n",
    "Alternatively you can pass a dictionary that maps each output name to the corresponding loss. Just like for the inputs, this is useful when there are multiple outputs, to avoid getting the order wrong. The loss weights and metrics(discussed shortly) can also be set using dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=['mse', 'mse'], loss_weights=[0.9, 0.1], optimizer='sgd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we train the model, we need to provide labels for each output. In this example, the main output and the auxiliary output should try to predict the same thing, so they should use the same labels. So instead of passing y_train we should pass (y_train, y_train) (and the same goes for y_valid and y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 2s 160us/sample - loss: 1.1179 - main_output_loss: 0.9675 - aux_output_loss: 2.4706 - val_loss: 0.8007 - val_main_output_loss: 0.7349 - val_aux_output_loss: 1.3921\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 1s 77us/sample - loss: 0.5979 - main_output_loss: 0.5273 - aux_output_loss: 1.2336 - val_loss: 0.5343 - val_main_output_loss: 0.4671 - val_aux_output_loss: 1.1398\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 1s 70us/sample - loss: 0.5188 - main_output_loss: 0.4579 - aux_output_loss: 1.0669 - val_loss: 0.4867 - val_main_output_loss: 0.4310 - val_aux_output_loss: 0.9879\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 1s 70us/sample - loss: 0.4864 - main_output_loss: 0.4364 - aux_output_loss: 0.9367 - val_loss: 0.4579 - val_main_output_loss: 0.4121 - val_aux_output_loss: 0.8706\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 1s 71us/sample - loss: 0.4652 - main_output_loss: 0.4239 - aux_output_loss: 0.8381 - val_loss: 0.4396 - val_main_output_loss: 0.4017 - val_aux_output_loss: 0.7803\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 1s 70us/sample - loss: 0.4498 - main_output_loss: 0.4150 - aux_output_loss: 0.7619 - val_loss: 0.4277 - val_main_output_loss: 0.3951 - val_aux_output_loss: 0.7218\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 1s 72us/sample - loss: 0.4391 - main_output_loss: 0.4093 - aux_output_loss: 0.7078 - val_loss: 0.4173 - val_main_output_loss: 0.3896 - val_aux_output_loss: 0.6672\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 1s 73us/sample - loss: 0.4379 - main_output_loss: 0.4114 - aux_output_loss: 0.6765 - val_loss: 0.4278 - val_main_output_loss: 0.3999 - val_aux_output_loss: 0.6791\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 1s 86us/sample - loss: 0.4269 - main_output_loss: 0.4026 - aux_output_loss: 0.6468 - val_loss: 0.4001 - val_main_output_loss: 0.3772 - val_aux_output_loss: 0.6061\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 1s 85us/sample - loss: 0.4208 - main_output_loss: 0.4000 - aux_output_loss: 0.6064 - val_loss: 0.3949 - val_main_output_loss: 0.3745 - val_aux_output_loss: 0.5786\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 1s 81us/sample - loss: 0.4093 - main_output_loss: 0.3892 - aux_output_loss: 0.5890 - val_loss: 0.3902 - val_main_output_loss: 0.3709 - val_aux_output_loss: 0.5643\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 1s 72us/sample - loss: 0.3998 - main_output_loss: 0.3809 - aux_output_loss: 0.5704 - val_loss: 0.3848 - val_main_output_loss: 0.3665 - val_aux_output_loss: 0.5495\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 1s 73us/sample - loss: 0.4033 - main_output_loss: 0.3850 - aux_output_loss: 0.5666 - val_loss: 0.3810 - val_main_output_loss: 0.3631 - val_aux_output_loss: 0.5428\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 1s 77us/sample - loss: 0.3951 - main_output_loss: 0.3782 - aux_output_loss: 0.5472 - val_loss: 0.3793 - val_main_output_loss: 0.3630 - val_aux_output_loss: 0.5259\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 1s 74us/sample - loss: 0.3870 - main_output_loss: 0.3708 - aux_output_loss: 0.5319 - val_loss: 0.3693 - val_main_output_loss: 0.3533 - val_aux_output_loss: 0.5127\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 1s 72us/sample - loss: 0.3846 - main_output_loss: 0.3691 - aux_output_loss: 0.5247 - val_loss: 0.3688 - val_main_output_loss: 0.3536 - val_aux_output_loss: 0.5052\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 1s 72us/sample - loss: 0.3841 - main_output_loss: 0.3688 - aux_output_loss: 0.5225 - val_loss: 0.3657 - val_main_output_loss: 0.3508 - val_aux_output_loss: 0.5002\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 1s 73us/sample - loss: 0.3797 - main_output_loss: 0.3654 - aux_output_loss: 0.5080 - val_loss: 0.3685 - val_main_output_loss: 0.3545 - val_aux_output_loss: 0.4942\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 1s 74us/sample - loss: 0.3711 - main_output_loss: 0.3566 - aux_output_loss: 0.5011 - val_loss: 0.3710 - val_main_output_loss: 0.3569 - val_aux_output_loss: 0.4974\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 1s 79us/sample - loss: 0.3770 - main_output_loss: 0.3630 - aux_output_loss: 0.5048 - val_loss: 0.3577 - val_main_output_loss: 0.3437 - val_aux_output_loss: 0.4830\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=20,\n",
    "                    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we evaluate the model, Keras will return the total loss, as well as all the idividual losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5160/5160 [==============================] - 0s 40us/sample - loss: 0.3550 - main_output_loss: 0.3401 - aux_output_loss: 0.4818\n"
     ]
    }
   ],
   "source": [
    "total_loss, main_loss, aux_loss = model.evaluate([X_test_A, X_test_B], [y_test, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the predict() method will return predictions for each output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see you can build any type sort of architecture with Keras quite easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
