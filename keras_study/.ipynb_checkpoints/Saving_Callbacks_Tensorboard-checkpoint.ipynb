{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Restoring a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the Sequential API or the Functional API, saving a trained Keras model is as simple as it gets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000]/255.0, X_train_full[5000:]/255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape = X_train.shape[1:]),\n",
    "    keras.layers.Dense(300, activation='relu'),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/30\n",
      "55000/55000 [==============================] - 6s 111us/sample - loss: 0.7275 - accuracy: 0.7600 - val_loss: 0.5127 - val_accuracy: 0.8274\n",
      "Epoch 2/30\n",
      "55000/55000 [==============================] - 5s 90us/sample - loss: 0.4909 - accuracy: 0.8295 - val_loss: 0.4561 - val_accuracy: 0.8438\n",
      "Epoch 3/30\n",
      "55000/55000 [==============================] - 5s 90us/sample - loss: 0.4454 - accuracy: 0.8436 - val_loss: 0.4163 - val_accuracy: 0.8570\n",
      "Epoch 4/30\n",
      "55000/55000 [==============================] - 6s 108us/sample - loss: 0.4179 - accuracy: 0.8525 - val_loss: 0.3977 - val_accuracy: 0.8630\n",
      "Epoch 5/30\n",
      "55000/55000 [==============================] - 6s 107us/sample - loss: 0.3975 - accuracy: 0.8601 - val_loss: 0.3956 - val_accuracy: 0.8596\n",
      "Epoch 6/30\n",
      "55000/55000 [==============================] - 5s 84us/sample - loss: 0.3791 - accuracy: 0.8653 - val_loss: 0.3824 - val_accuracy: 0.8702\n",
      "Epoch 7/30\n",
      "55000/55000 [==============================] - 5s 87us/sample - loss: 0.3659 - accuracy: 0.8708 - val_loss: 0.3721 - val_accuracy: 0.8694\n",
      "Epoch 8/30\n",
      "55000/55000 [==============================] - 5s 92us/sample - loss: 0.3538 - accuracy: 0.8749 - val_loss: 0.3576 - val_accuracy: 0.8740\n",
      "Epoch 9/30\n",
      "55000/55000 [==============================] - 5s 87us/sample - loss: 0.3439 - accuracy: 0.8779 - val_loss: 0.3475 - val_accuracy: 0.8760\n",
      "Epoch 10/30\n",
      "55000/55000 [==============================] - 5s 87us/sample - loss: 0.3335 - accuracy: 0.8805 - val_loss: 0.3512 - val_accuracy: 0.8754\n",
      "Epoch 11/30\n",
      "55000/55000 [==============================] - 6s 105us/sample - loss: 0.3256 - accuracy: 0.8835 - val_loss: 0.3419 - val_accuracy: 0.8820\n",
      "Epoch 12/30\n",
      "55000/55000 [==============================] - 6s 103us/sample - loss: 0.3178 - accuracy: 0.8858 - val_loss: 0.3350 - val_accuracy: 0.8776\n",
      "Epoch 13/30\n",
      "55000/55000 [==============================] - 5s 86us/sample - loss: 0.3097 - accuracy: 0.8882 - val_loss: 0.3278 - val_accuracy: 0.8830\n",
      "Epoch 14/30\n",
      "55000/55000 [==============================] - 6s 104us/sample - loss: 0.3032 - accuracy: 0.8906 - val_loss: 0.3494 - val_accuracy: 0.8730\n",
      "Epoch 15/30\n",
      "55000/55000 [==============================] - 5s 94us/sample - loss: 0.2974 - accuracy: 0.8930 - val_loss: 0.3259 - val_accuracy: 0.8832\n",
      "Epoch 16/30\n",
      "55000/55000 [==============================] - 5s 90us/sample - loss: 0.2903 - accuracy: 0.8960 - val_loss: 0.3141 - val_accuracy: 0.8858\n",
      "Epoch 17/30\n",
      "55000/55000 [==============================] - 5s 99us/sample - loss: 0.2850 - accuracy: 0.8966 - val_loss: 0.3285 - val_accuracy: 0.8788\n",
      "Epoch 18/30\n",
      "55000/55000 [==============================] - 5s 87us/sample - loss: 0.2782 - accuracy: 0.8990 - val_loss: 0.3179 - val_accuracy: 0.8872\n",
      "Epoch 19/30\n",
      "55000/55000 [==============================] - 5s 89us/sample - loss: 0.2732 - accuracy: 0.9012 - val_loss: 0.3100 - val_accuracy: 0.8862\n",
      "Epoch 20/30\n",
      "55000/55000 [==============================] - 5s 89us/sample - loss: 0.2675 - accuracy: 0.9036 - val_loss: 0.3156 - val_accuracy: 0.8856\n",
      "Epoch 21/30\n",
      "55000/55000 [==============================] - 5s 92us/sample - loss: 0.2632 - accuracy: 0.9050 - val_loss: 0.3074 - val_accuracy: 0.8884\n",
      "Epoch 22/30\n",
      "55000/55000 [==============================] - 5s 87us/sample - loss: 0.2589 - accuracy: 0.9077 - val_loss: 0.3077 - val_accuracy: 0.8892\n",
      "Epoch 23/30\n",
      "55000/55000 [==============================] - 5s 92us/sample - loss: 0.2543 - accuracy: 0.9083 - val_loss: 0.3052 - val_accuracy: 0.8874\n",
      "Epoch 24/30\n",
      "55000/55000 [==============================] - 5s 89us/sample - loss: 0.2499 - accuracy: 0.9091 - val_loss: 0.3113 - val_accuracy: 0.8876\n",
      "Epoch 25/30\n",
      "55000/55000 [==============================] - 5s 88us/sample - loss: 0.2454 - accuracy: 0.9108 - val_loss: 0.3064 - val_accuracy: 0.8886\n",
      "Epoch 26/30\n",
      "55000/55000 [==============================] - 5s 88us/sample - loss: 0.2418 - accuracy: 0.9127 - val_loss: 0.3009 - val_accuracy: 0.8904\n",
      "Epoch 27/30\n",
      "55000/55000 [==============================] - 7s 119us/sample - loss: 0.2377 - accuracy: 0.9135 - val_loss: 0.3103 - val_accuracy: 0.8854\n",
      "Epoch 28/30\n",
      "55000/55000 [==============================] - 6s 108us/sample - loss: 0.2336 - accuracy: 0.9157 - val_loss: 0.2948 - val_accuracy: 0.8948\n",
      "Epoch 29/30\n",
      "55000/55000 [==============================] - 6s 105us/sample - loss: 0.2298 - accuracy: 0.9162 - val_loss: 0.2935 - val_accuracy: 0.8936\n",
      "Epoch 30/30\n",
      "55000/55000 [==============================] - 6s 105us/sample - loss: 0.2257 - accuracy: 0.9193 - val_loss: 0.2953 - val_accuracy: 0.8918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x154f9a748>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_keras_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras will use the HDF5 format to save both the model's arhitecture (including every layer's hyperparameters) and the values of all the model parameters for every layer (e.g., connection weights and biases). It also saves the optimizer (including its hyperparameters and any state it may have). You will typically have a script that trains a model and saves it, and one or more scripts (or web services) that load the model and use it to make predictions. Loading the model is just as easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('my_keras_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will work when using the Sequential Api or the Fucntional API, but unfortunately not when using model subclassing. You can use save_weights() and load_weights() to at least save and restore the model parameters, but you will need to save and restore everything yourself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if training lasts several hours? This is quite common, especially when training on large datasets. In this case, you should not only save your model at the end of training , but also save checkpoints at regular intervals during training, to avoid losing everything if your computer crashes. But how can you tell the fit() method to save checkpoints? Use callbacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit() method accepts a callbacks argument that lets you specify a list of objects that Keras will call at the start and at the end of training , at the start and end of each epoch, and even before and after processing each batch. For example, the ModelCheckpoint callback saves checkpoints of your model at regular intervals during training, by default at the end of each epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples\n",
      "Epoch 1/10\n",
      "55000/55000 [==============================] - 5s 90us/sample - loss: 0.7169 - accuracy: 0.7647\n",
      "Epoch 2/10\n",
      "55000/55000 [==============================] - 5s 86us/sample - loss: 0.4891 - accuracy: 0.8296\n",
      "Epoch 3/10\n",
      "55000/55000 [==============================] - 5s 83us/sample - loss: 0.4435 - accuracy: 0.8446\n",
      "Epoch 4/10\n",
      "55000/55000 [==============================] - 5s 84us/sample - loss: 0.4151 - accuracy: 0.8539\n",
      "Epoch 5/10\n",
      "55000/55000 [==============================] - 5s 96us/sample - loss: 0.3952 - accuracy: 0.8603\n",
      "Epoch 6/10\n",
      "55000/55000 [==============================] - 5s 97us/sample - loss: 0.3790 - accuracy: 0.8669\n",
      "Epoch 7/10\n",
      "55000/55000 [==============================] - 5s 89us/sample - loss: 0.3650 - accuracy: 0.8702\n",
      "Epoch 8/10\n",
      "55000/55000 [==============================] - 5s 95us/sample - loss: 0.3545 - accuracy: 0.8746\n",
      "Epoch 9/10\n",
      "55000/55000 [==============================] - 5s 83us/sample - loss: 0.3440 - accuracy: 0.8783\n",
      "Epoch 10/10\n",
      "55000/55000 [==============================] - 4s 81us/sample - loss: 0.3342 - accuracy: 0.8816\n"
     ]
    }
   ],
   "source": [
    "model_0 = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape = X_train.shape[1:]),\n",
    "    keras.layers.Dense(300, activation='relu'),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model_0.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model_0.h5\")\n",
    "history_0 = model_0.fit(X_train, y_train, epochs=10, callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Morove if you use a validation set during training you can use save_best_only = True when creating the ModelCheckpoint. In this case, it will only save your model when it's performance on the validation set is the best so far. This way, you do not need to worry about training for too long and overfitting the training set: simply restore the last model saved after training, and this will be the best model on the validation set. The following code is simple way to implement early stopping:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "55000/55000 [==============================] - 6s 101us/sample - loss: 0.7177 - accuracy: 0.7640 - val_loss: 0.5203 - val_accuracy: 0.8262\n",
      "Epoch 2/10\n",
      "55000/55000 [==============================] - 6s 113us/sample - loss: 0.4892 - accuracy: 0.8291 - val_loss: 0.4392 - val_accuracy: 0.8494\n",
      "Epoch 3/10\n",
      "55000/55000 [==============================] - 6s 114us/sample - loss: 0.4447 - accuracy: 0.8441 - val_loss: 0.4171 - val_accuracy: 0.8612\n",
      "Epoch 4/10\n",
      "55000/55000 [==============================] - 8s 140us/sample - loss: 0.4182 - accuracy: 0.8524 - val_loss: 0.3967 - val_accuracy: 0.8642\n",
      "Epoch 5/10\n",
      "55000/55000 [==============================] - 6s 118us/sample - loss: 0.3988 - accuracy: 0.8594 - val_loss: 0.3854 - val_accuracy: 0.8690\n",
      "Epoch 6/10\n",
      "55000/55000 [==============================] - 7s 122us/sample - loss: 0.3806 - accuracy: 0.8652 - val_loss: 0.3828 - val_accuracy: 0.8656\n",
      "Epoch 7/10\n",
      "55000/55000 [==============================] - 6s 101us/sample - loss: 0.3675 - accuracy: 0.8701 - val_loss: 0.3667 - val_accuracy: 0.8762\n",
      "Epoch 8/10\n",
      "55000/55000 [==============================] - 6s 106us/sample - loss: 0.3559 - accuracy: 0.8740 - val_loss: 0.3621 - val_accuracy: 0.8752\n",
      "Epoch 9/10\n",
      "55000/55000 [==============================] - 6s 117us/sample - loss: 0.3459 - accuracy: 0.8766 - val_loss: 0.3598 - val_accuracy: 0.8704\n",
      "Epoch 10/10\n",
      "55000/55000 [==============================] - 6s 103us/sample - loss: 0.3355 - accuracy: 0.8807 - val_loss: 0.3493 - val_accuracy: 0.8776\n"
     ]
    }
   ],
   "source": [
    "model_1 = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape = X_train.shape[1:]),\n",
    "    keras.layers.Dense(300, activation='relu'),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model_1.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('my_keras_model_1.h5', save_best_only=True)\n",
    "history_1 = model_1.fit(X_train, y_train,epochs=10,validation_data=(X_valid, y_valid),callbacks=[checkpoint_cb])\n",
    "model = keras.models.load_model('my_keras_model_1.h5') #roll back to best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to implement early stopping is to simply use the EarlyStopping callback. It will interrupt training when it measures no progress on the validation set for a number of epoch (defined by the patience argument), and it will optionally roll back to the best model. You can combine both callbacks to checkpoints of your model (in case your computer crashes) and interrupt training early when there is no more progress (to avoid wasting time and resources):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "55000/55000 [==============================] - 10s 182us/sample - loss: 0.7354 - accuracy: 0.7575 - val_loss: 0.5019 - val_accuracy: 0.8352\n",
      "Epoch 2/100\n",
      "55000/55000 [==============================] - 7s 124us/sample - loss: 0.4977 - accuracy: 0.8274 - val_loss: 0.4778 - val_accuracy: 0.8284\n",
      "Epoch 3/100\n",
      "55000/55000 [==============================] - 6s 105us/sample - loss: 0.4495 - accuracy: 0.8424 - val_loss: 0.4313 - val_accuracy: 0.8542\n",
      "Epoch 4/100\n",
      "55000/55000 [==============================] - 6s 112us/sample - loss: 0.4214 - accuracy: 0.8517 - val_loss: 0.4063 - val_accuracy: 0.8584\n",
      "Epoch 5/100\n",
      "55000/55000 [==============================] - 7s 124us/sample - loss: 0.4005 - accuracy: 0.8599 - val_loss: 0.3849 - val_accuracy: 0.8666\n",
      "Epoch 6/100\n",
      "55000/55000 [==============================] - 6s 103us/sample - loss: 0.3860 - accuracy: 0.8635 - val_loss: 0.3915 - val_accuracy: 0.8654\n",
      "Epoch 7/100\n",
      "55000/55000 [==============================] - 5s 97us/sample - loss: 0.3710 - accuracy: 0.8684 - val_loss: 0.3868 - val_accuracy: 0.8636\n",
      "Epoch 8/100\n",
      "55000/55000 [==============================] - 5s 93us/sample - loss: 0.3582 - accuracy: 0.8732 - val_loss: 0.3566 - val_accuracy: 0.8770\n",
      "Epoch 9/100\n",
      "55000/55000 [==============================] - 5s 94us/sample - loss: 0.3494 - accuracy: 0.8760 - val_loss: 0.3652 - val_accuracy: 0.8742\n",
      "Epoch 10/100\n",
      "55000/55000 [==============================] - 5s 92us/sample - loss: 0.3392 - accuracy: 0.8796 - val_loss: 0.3494 - val_accuracy: 0.8762\n",
      "Epoch 11/100\n",
      "55000/55000 [==============================] - 5s 97us/sample - loss: 0.3297 - accuracy: 0.8830 - val_loss: 0.3366 - val_accuracy: 0.8820\n",
      "Epoch 12/100\n",
      "55000/55000 [==============================] - 5s 89us/sample - loss: 0.3218 - accuracy: 0.8855 - val_loss: 0.3478 - val_accuracy: 0.8776\n",
      "Epoch 13/100\n",
      "55000/55000 [==============================] - 5s 90us/sample - loss: 0.3149 - accuracy: 0.8879 - val_loss: 0.3291 - val_accuracy: 0.8848\n",
      "Epoch 14/100\n",
      "55000/55000 [==============================] - 5s 93us/sample - loss: 0.3076 - accuracy: 0.8898 - val_loss: 0.3218 - val_accuracy: 0.8870\n",
      "Epoch 15/100\n",
      "55000/55000 [==============================] - 5s 89us/sample - loss: 0.3006 - accuracy: 0.8929 - val_loss: 0.3329 - val_accuracy: 0.8802\n",
      "Epoch 16/100\n",
      "55000/55000 [==============================] - 5s 90us/sample - loss: 0.2940 - accuracy: 0.8949 - val_loss: 0.3187 - val_accuracy: 0.8912\n",
      "Epoch 17/100\n",
      "55000/55000 [==============================] - 5s 92us/sample - loss: 0.2886 - accuracy: 0.8975 - val_loss: 0.3406 - val_accuracy: 0.8750\n",
      "Epoch 18/100\n",
      "55000/55000 [==============================] - 5s 94us/sample - loss: 0.2821 - accuracy: 0.8997 - val_loss: 0.3251 - val_accuracy: 0.8818\n",
      "Epoch 19/100\n",
      "55000/55000 [==============================] - 5s 94us/sample - loss: 0.2775 - accuracy: 0.8996 - val_loss: 0.3421 - val_accuracy: 0.8788\n",
      "Epoch 20/100\n",
      "55000/55000 [==============================] - 5s 96us/sample - loss: 0.2718 - accuracy: 0.9030 - val_loss: 0.3083 - val_accuracy: 0.8892\n",
      "Epoch 21/100\n",
      "55000/55000 [==============================] - 6s 107us/sample - loss: 0.2664 - accuracy: 0.9047 - val_loss: 0.3493 - val_accuracy: 0.8754\n",
      "Epoch 22/100\n",
      "55000/55000 [==============================] - 6s 101us/sample - loss: 0.2623 - accuracy: 0.9058 - val_loss: 0.3346 - val_accuracy: 0.8790\n",
      "Epoch 23/100\n",
      "55000/55000 [==============================] - 5s 99us/sample - loss: 0.2568 - accuracy: 0.9081 - val_loss: 0.3056 - val_accuracy: 0.8918\n",
      "Epoch 24/100\n",
      "55000/55000 [==============================] - 5s 92us/sample - loss: 0.2533 - accuracy: 0.9093 - val_loss: 0.3155 - val_accuracy: 0.8868\n",
      "Epoch 25/100\n",
      "55000/55000 [==============================] - 5s 94us/sample - loss: 0.2479 - accuracy: 0.9105 - val_loss: 0.3057 - val_accuracy: 0.8918\n",
      "Epoch 26/100\n",
      "55000/55000 [==============================] - 5s 97us/sample - loss: 0.2445 - accuracy: 0.9132 - val_loss: 0.2998 - val_accuracy: 0.8920\n",
      "Epoch 27/100\n",
      "55000/55000 [==============================] - 6s 100us/sample - loss: 0.2392 - accuracy: 0.9142 - val_loss: 0.2961 - val_accuracy: 0.8938\n",
      "Epoch 28/100\n",
      "55000/55000 [==============================] - 5s 91us/sample - loss: 0.2356 - accuracy: 0.9165 - val_loss: 0.3354 - val_accuracy: 0.8808\n",
      "Epoch 29/100\n",
      "55000/55000 [==============================] - 5s 98us/sample - loss: 0.2316 - accuracy: 0.9164 - val_loss: 0.3192 - val_accuracy: 0.8848\n",
      "Epoch 30/100\n",
      "55000/55000 [==============================] - 5s 93us/sample - loss: 0.2269 - accuracy: 0.9191 - val_loss: 0.2988 - val_accuracy: 0.8922\n",
      "Epoch 31/100\n",
      "55000/55000 [==============================] - 5s 97us/sample - loss: 0.2241 - accuracy: 0.9202 - val_loss: 0.3022 - val_accuracy: 0.8904\n",
      "Epoch 32/100\n",
      "55000/55000 [==============================] - 5s 95us/sample - loss: 0.2208 - accuracy: 0.9208 - val_loss: 0.2927 - val_accuracy: 0.8932\n",
      "Epoch 33/100\n",
      "55000/55000 [==============================] - 5s 97us/sample - loss: 0.2173 - accuracy: 0.9213 - val_loss: 0.2886 - val_accuracy: 0.8932\n",
      "Epoch 34/100\n",
      "55000/55000 [==============================] - 5s 93us/sample - loss: 0.2138 - accuracy: 0.9240 - val_loss: 0.2948 - val_accuracy: 0.8942\n",
      "Epoch 35/100\n",
      "55000/55000 [==============================] - 5s 93us/sample - loss: 0.2098 - accuracy: 0.9258 - val_loss: 0.3072 - val_accuracy: 0.8894\n",
      "Epoch 36/100\n",
      "55000/55000 [==============================] - 5s 92us/sample - loss: 0.2064 - accuracy: 0.9268 - val_loss: 0.2975 - val_accuracy: 0.8926\n",
      "Epoch 37/100\n",
      "55000/55000 [==============================] - 5s 92us/sample - loss: 0.2033 - accuracy: 0.9283 - val_loss: 0.3045 - val_accuracy: 0.8916\n",
      "Epoch 38/100\n",
      "55000/55000 [==============================] - 5s 99us/sample - loss: 0.2003 - accuracy: 0.9286 - val_loss: 0.3007 - val_accuracy: 0.8928\n",
      "Epoch 39/100\n",
      "55000/55000 [==============================] - 6s 107us/sample - loss: 0.1973 - accuracy: 0.9302 - val_loss: 0.2932 - val_accuracy: 0.8944\n",
      "Epoch 40/100\n",
      "55000/55000 [==============================] - 5s 94us/sample - loss: 0.1940 - accuracy: 0.9309 - val_loss: 0.2999 - val_accuracy: 0.8944\n",
      "Epoch 41/100\n",
      "55000/55000 [==============================] - 5s 96us/sample - loss: 0.1920 - accuracy: 0.9324 - val_loss: 0.2901 - val_accuracy: 0.8954\n",
      "Epoch 42/100\n",
      "55000/55000 [==============================] - 5s 92us/sample - loss: 0.1874 - accuracy: 0.9332 - val_loss: 0.2950 - val_accuracy: 0.8956\n",
      "Epoch 43/100\n",
      "55000/55000 [==============================] - 5s 93us/sample - loss: 0.1866 - accuracy: 0.9331 - val_loss: 0.3004 - val_accuracy: 0.8954\n"
     ]
    }
   ],
   "source": [
    "model_2 = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape = X_train.shape[1:]),\n",
    "    keras.layers.Dense(300, activation='relu'),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model_2.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "checkpoint_cb_2 = keras.callbacks.ModelCheckpoint('my_keras_model_2.h5', save_best_only=True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "history_2 = model_2.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid),\n",
    "                        callbacks=[checkpoint_cb_2, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of epochs can be set to a large value since training will stop automatically when there is no more progress. In this case, there is no need to restore the best model saved because the EarlyStopping callback will keep track of the best weights and restore them for you at the end of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is many other callbacks available in the keras.callbacks package (https://keras.io/callbacks/.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need extra control, you can easily write your own custom callbacks. As an example of how to do that, \n",
    "the following custom callback will display the ratio between the validation loss and training loss during training(e.g. to detect overfitting):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print('\\nval/train: {:2f}'.format(logs['val_loss']/logs['loss']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might expect you can implement on_train_begin(), on_train_end(), on_epoch_begin(), on_epoch_end(),on_batc_begin(),on_natch_end(). Callbacks can also be used during evaluation and predictions, should you ever need them on_test_end(), on_test_batch_begin(), or on_test_batch_end() (called by evaluate()), and for prediction you should implement on_predict_begin(), on_predict_end(), on_predict_batch_begin(), or on_predict_batch_end() (called by predict())."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at one more tool from tf.keras: TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TensorBoard for Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard is a great interactive visualization tool that you can use to view the learning curves during training, compare learning curves between multiple runs, visualize the computation graph, analyze training statistics, view images generated by your model, visualize complex multidimensional data projected down to 3D and automatically clustered for you, and more! This tool is installed automatically when you install TensorFlow, so you already have it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use it, you must modify your program so that it outputs the data you want to visualize to special binary\n",
    "log files called event files. Each binary data record is called a summary. The TensorBoard server will monitor the log directory, and it will automatically pick up the changes and update the visualizations: this allow you to visualize live data(with a short delay), such as the learning curves during training. In general, you want to point the TensorBoard server instance will allow you to visualize and compare data from multiple runs of your program, without getting everything mixed up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining the root log directory we will use for our TensorBoard logs, plus a small funtion that will\n",
    "generate a subdirectory path based on the current date and time so that it's different at every run. you may want to include extra information in the log directory name, such as hyperparameter values you are testing, to make it easier to know what you are looking at in TensorBoard:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# root_logdir = os.path.join(os.curdir, 'my_logs')\n",
    "\n",
    "# def get_run_logdir():\n",
    "#     import time\n",
    "#     run_id = \"logs/scalars/\" + time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "#     return os.path.join(root_logdir, run_id)\n",
    "\n",
    "# run_logdir = get_run_logdir() # e.g., './my_logs/run_2019_06_07-15_15_22'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The good news is that Keras provides a nice TensorBoard() callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test loss:  0.5213567116247524\n"
     ]
    }
   ],
   "source": [
    "# run_logdir = get_run_logdir() can be used with the code mentioned above, the code provided below is  from \n",
    "# tensorflow's own documentation.\n",
    "\n",
    "logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "model_3 = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape = X_train.shape[1:]),\n",
    "    keras.layers.Dense(300, activation='relu'),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "model_3.compile(\n",
    "    loss='sparse_categorical_crossentropy', # keras.losses.sparse_categorical_crossentropy\n",
    "    optimizer=keras.optimizers.SGD(lr=0.001),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "training_history = model_3.fit(\n",
    "    X_train, # input\n",
    "    y_train, # output\n",
    "    verbose=0, # Suppress chatty output; use Tensorboard instead\n",
    "    epochs=30,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=[tensorboard_callback],\n",
    ")\n",
    "\n",
    "print(\"Average test loss: \", np.average(training_history.history['loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test loss:  0.5213567116247524\n"
     ]
    }
   ],
   "source": [
    "logdir0 = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback_4 = keras.callbacks.TensorBoard(log_dir=logdir0)\n",
    "model_4 = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape = X_train.shape[1:]),\n",
    "    keras.layers.Dense(300, activation='relu'),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "model_4.compile(\n",
    "    loss='sparse_categorical_crossentropy', # keras.losses.sparse_categorical_crossentropy\n",
    "    optimizer=keras.optimizers.SGD(lr=0.05),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "training_history_4 = model_4.fit(\n",
    "    X_train, # input\n",
    "    y_train, # output\n",
    "    verbose=0, # Suppress chatty output; use Tensorboard instead\n",
    "    epochs=30,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=[tensorboard_callback_4],\n",
    ")\n",
    "\n",
    "print(\"Average test loss: \", np.average(training_history.history['loss']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's all there is to it! It could hardly be easier to use. If you run this code, the TensorBoard() callback\n",
    "will take care of creating the log directory for you (along  with its parent directories if needed), and during training it will create event files and summaries to them. After running the program a second time (perhaps changing some hyperparameter value), you will end up with a directory structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's one directory per run, each containing one subdirectory for training logs and one for validation logs. Both contain event files, but the training logs also include profiling traces: this allows TensorBoard to show you exactly how much time the model spent on each part of your model, across all your devices, which is great for locating performance bottlenecks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you need to start the TensorBoard server. One way to do this by running a command in a terminal. If you installed within a virtualenv, you should activate it. Next, run the following command at the root of the project (or from anywhere else, as long as you point to the appropriate log directory):\n",
    "\n",
    "\n",
    "$ tensorboard --logdir=./my_logs --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your shell cannot find the tensorboard script, then you must update your PATH environment variable so that it contains the directory in which the script was installed (alternatively, you can just replace tensorboard in the command line with python3 -m tensorboard.main). Once server is up, you can open a web browser and go to http://localhost:6006."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can use TensorBoard directly within Jupyter, by running the following commands. The first line \n",
    "loads the TensorBoard extension, and the second line starts a TensorBoard server on port 6006 (unless it is already started) and connects to it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either way, you should see TensorBoard's web interface. Click the SCALARS tab to view the learning curves. At the bottom left, select the logs you want to visualize (e.g., the training logs from the first and second run), and click the epoch_loss scalar. Notice that the training loss went down nicely during both runs, but the second run went down much faster. Indeed, we used a learning rate of 0.05 (optimizer=keras.optimizers.SGD(lr=0.05)) instead of 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 30456), started 0:35:19 ago. (Use '!kill 30456' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-be7dc0543728ce48\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-be7dc0543728ce48\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6007;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./logs/scalars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that sometimes Keras callback creating .profile-empty file blocks loading data, the same problem occurs in tf-nightly (non-2.0-preview), but manifests differently: because there is only one run (named .) instead\n",
    "of separate train/validation, all data stops being displayed after the epoch in which TensorBoard is opened.\n",
    "\n",
    "Note as a special case of this that if TensorBoard is running before training starts, then train data may not appear at all. \n",
    "\n",
    "It should be fine by killing and reloading the TensorBoard, otherwise get more infor from https://github.com/tensorflow/tensorboard/issues/2084 to tackle with the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 30782), started 0:00:13 ago. (Use '!kill 30782' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-cdb2dd8512410f25\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-cdb2dd8512410f25\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6007;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!kill it with the number provided\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=./logs/scalars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also visualize the whole graph, the learned weights (projected to 3D), or the profiling traces. The TensorBoard() callback has options to log extra data too, such as embeddings (see Chapter 13).\n",
    "\n",
    "Additionally, TensorFlow offers a lower-level API in the tf.summary package. The following code creates a SummaryWriter using the create_file_writer() function, and it uses this writer as a context to log scalars, histograms, images, audio, and text, all of which can than be visualized using TensorBoard(give it a try!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "writer = tf.summary.create_file_writer(test_logdir)\n",
    "with writer.as_default():\n",
    "    for step in range(1,1000 + 1):\n",
    "        tf.summary.scalar('my_scalar', np.sin(step/10), step=step)\n",
    "        data = (np.random.rand(100) + 2)*step/100 #some random data\n",
    "        tf.summary.histogram('my_list', data, buckets=50, step=step)\n",
    "        images = np.random.rand(2,32,32,3) #random 32*32 RGB images\n",
    "        tf.summary.image('my_images',images*step/1000, step=step)\n",
    "        texts = ['The step is ' + str(step), \"It's square is \" + str(step**2)]\n",
    "        tf.summary.text('my_text', texts, step=step)\n",
    "        sine_wave = tf.math.sin(tf.range(12000)/48000*2*np.pi*step)\n",
    "        audio = tf.reshape(tf.cast(sine_wave, tf.float32), [1,-1,1])\n",
    "        tf.summary.audio('my_audio', audio, sample_rate=48000, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-a40d0e0e66c818d7\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-a40d0e0e66c818d7\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6007;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!kill 30456\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=./logs/scalars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is actually a useful visualization tool to have, even beyond Tensorflow or Deep Learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
